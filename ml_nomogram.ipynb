{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98280a5a-db1f-4890-888a-368cd12b7547",
   "metadata": {},
   "source": [
    "# Nomogram for a machine learning model with categorical predictors to predict binary outcome\n",
    "**Author: Herdiantri Sufriyana, Emily Chia-Yu Su**  \n",
    "*Date: 2024-01-15*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ffd0a-f363-4150-9dad-4714a03fb696",
   "metadata": {},
   "source": [
    "## Programming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e7b724-e2fd-4a96-a7e7-533b485cc25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from joblib import dump, parallel_backend, load\n",
    "import os\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ffc34e-f60e-4cc7-ac3a-4d8d02e3e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 2008\n"
     ]
    }
   ],
   "source": [
    "seed=2024-1-15\n",
    "\n",
    "print(f'Seed: {seed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edf547-30b0-4ad5-832d-62237eaee6e1",
   "metadata": {},
   "source": [
    "## Input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78f37ef-f0c9-4d35-a845-67e2d7b678c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=[\n",
    "    'training'\n",
    "    ,'validation'\n",
    "    ,'test'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d745f6d-c0a1-430d-90f7-939667b079ec",
   "metadata": {},
   "source": [
    "## Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8552b4e6-ae70-4755-ac5f-f0990b4d1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms to be used\n",
    "algorithms={\n",
    "    'rr':LogisticRegression(penalty='l2',solver='saga',random_state=seed,max_iter=1000)\n",
    "    ,'rf':RandomForestClassifier(random_state=seed)\n",
    "    ,'gb':XGBClassifier(eval_metric='logloss',random_state=seed)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151dc29a-ea62-465a-9a32-ac2c6d372e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for each algorithm\n",
    "hyperparam_grids={\n",
    "    'rr':{\n",
    "        'C':[0.01,0.1,1,10]\n",
    "    }\n",
    "    ,'rf':{\n",
    "        'n_estimators':[10,50,100]\n",
    "        ,'max_depth':[None,10,20]\n",
    "        ,'min_samples_split':[2,5,10]\n",
    "    }\n",
    "    ,'gb':{\n",
    "        'n_estimators':[50,100,200]\n",
    "        ,'learning_rate':[0.01,0.1,1]\n",
    "        ,'max_depth':[3,4,5]\n",
    "        ,'subsample':[0.8,1]\n",
    "        ,'colsample_bytree':[0.8,1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9c6954-0b21-4c3d-9658-8893f92d2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation with random seed\n",
    "cv=StratifiedKFold(n_splits=10,shuffle=True,random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28909c29-c313-4ccf-b534-52f08217f2a1",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c065e08-faf4-454d-bfb4-4f78e9cef162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file: data/model_input/training.csv, Algorithm: rr, Prediction file: data/model_input/training.csv\n",
      "AUC-ROC: 0.9681840268574963\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: rr, Prediction file: data/model_input/validation.csv\n",
      "AUC-ROC: 0.9844074844074844\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: rr, Prediction file: data/model_input/test.csv\n",
      "AUC-ROC: 0.9717261904761905\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: rf, Prediction file: data/model_input/training.csv\n",
      "AUC-ROC: 0.9729768530788938\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: rf, Prediction file: data/model_input/validation.csv\n",
      "AUC-ROC: 0.9792099792099792\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: rf, Prediction file: data/model_input/test.csv\n",
      "AUC-ROC: 0.9673763736263736\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: gb, Prediction file: data/model_input/training.csv\n",
      "AUC-ROC: 0.9704147892923403\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: gb, Prediction file: data/model_input/validation.csv\n",
      "AUC-ROC: 0.9844074844074844\n",
      "\n",
      "Training file: data/model_input/training.csv, Algorithm: gb, Prediction file: data/model_input/test.csv\n",
      "AUC-ROC: 0.97378663003663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "training_file=f\"data/model_input/training.csv\"\n",
    "training_data=pd.read_csv(training_file)\n",
    "\n",
    "# Extract features, labels, and sample weights from the training data\n",
    "X_train=training_data.drop(columns=['outcome','outcome_weight'])\n",
    "y_train=training_data['outcome']\n",
    "sample_weights=training_data['outcome_weight']\n",
    "\n",
    "# Loop through each algorithm\n",
    "for algorithm_name,algorithm in algorithms.items():\n",
    "    \n",
    "    # Define the hyperparameter grid\n",
    "    hyperparam_grid=hyperparam_grids[algorithm_name]\n",
    "    \n",
    "    # Train the model with cross-validated grid search and sample weighting\n",
    "    grid_search=GridSearchCV(algorithm,hyperparam_grid,cv=cv,n_jobs=14,scoring='roc_auc')\n",
    "    \n",
    "    with parallel_backend('threading',n_jobs=14):\n",
    "        grid_search.fit(X_train,y_train,sample_weight=sample_weights)\n",
    "           \n",
    "    # Get the best model and best hyperparameters from the grid search\n",
    "    best_model=grid_search.best_estimator_\n",
    "    best_hyperparams=grid_search.best_params_\n",
    "    \n",
    "    # Define the directory to save the models\n",
    "    save_dir=f\"data/sklearn_models/{algorithm_name}\"\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    model_filename=f\"{save_dir}/model.joblib\"\n",
    "    dump(best_model,model_filename)\n",
    "    \n",
    "    # Save the best hyperparameters to a CSV file\n",
    "    hyperparams_df=pd.DataFrame([best_hyperparams])\n",
    "    hyperparams_filename=f\"{save_dir}/best_hyperparams.csv\"\n",
    "    hyperparams_df.to_csv(hyperparams_filename,index=False)\n",
    "            \n",
    "    # Loop through each prediction set and make predictions\n",
    "    for dataset in datasets:\n",
    "        \n",
    "        prediction_file=f\"data/model_input/{dataset}.csv\"\n",
    "\n",
    "        # Load the prediction dataset\n",
    "        prediction_data=pd.read_csv(prediction_file)\n",
    "        \n",
    "        # Extract features from the prediction data\n",
    "        X_pred=prediction_data.drop(columns=['outcome','outcome_weight'])\n",
    "        y_true=prediction_data['outcome']\n",
    "        \n",
    "        # Make predictions on the prediction data\n",
    "        y_pred_proba=best_model.predict_proba(X_pred)[:,1]\n",
    "        \n",
    "        # Output model evaluation\n",
    "        print(f\"Training file: {training_file}, Algorithm: {algorithm_name}, Prediction file: {prediction_file}\")\n",
    "        print(f\"AUC-ROC: {roc_auc_score(y_true,y_pred_proba)}\\n\")\n",
    "        \n",
    "        # Write the predicted probabilities to CSV\n",
    "        output_filename=f\"{save_dir}/prob_{dataset}.csv\"\n",
    "        pd.DataFrame({'outcome':y_true,'prob':y_pred_proba}).to_csv(output_filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2854cbfb-27b5-4eef-88b1-42fd7707f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding explainers\n",
    "explainers={\n",
    "    'rr':shap.LinearExplainer\n",
    "    ,'rf':shap.TreeExplainer\n",
    "    ,'gb':shap.TreeExplainer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95153246-93a4-48d7-8a1b-bc36bd22eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "training_file=f\"data/model_input/training.csv\"\n",
    "training_data=pd.read_csv(training_file)\n",
    "\n",
    "# Extract features, labels, and sample weights from the training data\n",
    "X_train=training_data.drop(columns=['outcome','outcome_weight'])\n",
    "\n",
    "# Loop through each algorithm\n",
    "for algorithm_name,algorithm in algorithms.items():\n",
    "    \n",
    "    # Define the directory to load the trained models\n",
    "    save_dir=f\"data/sklearn_models/{algorithm_name}\"\n",
    "    \n",
    "    # Load the trained model from a file\n",
    "    model_filename=os.path.join(save_dir,'model.joblib')\n",
    "    best_model=load(model_filename)\n",
    "    \n",
    "    # Initialize the SHAP explainer\n",
    "    explainer=explainers[algorithm_name](best_model,X_train)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values=explainer.shap_values(X_train)\n",
    "    \n",
    "    # Check if shap_values is a list (e.g., for binary classification problems)\n",
    "    if isinstance(shap_values,list):\n",
    "        # Select the SHAP values for the class of interest (e.g., class 1)\n",
    "        shap_values=shap_values[1]\n",
    "    \n",
    "    # Convert SHAP values to DataFrame and save to CSV\n",
    "    shap_csv_filename=os.path.join(save_dir,'shap_values.csv')\n",
    "    pd.DataFrame(shap_values,columns=X_train.columns).to_csv(shap_csv_filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b3c42e8-669c-48bb-9a46-408b43ee172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: rf, Prediction file: data/dataset_nomogram.csv\n"
     ]
    }
   ],
   "source": [
    "# Choose algorithm\n",
    "algorithm_name='rf'\n",
    "algorithm=RandomForestClassifier(random_state=seed)\n",
    "\n",
    "# Define the directory to load the trained models\n",
    "save_dir=f\"data/sklearn_models/{algorithm_name}\"\n",
    "\n",
    "# Load the trained model from a file\n",
    "model_filename=os.path.join(save_dir,'model.joblib')\n",
    "best_model=load(model_filename)\n",
    "\n",
    "# Choose prediction file\n",
    "prediction_file='data/pred_data_nomogram.csv'\n",
    "\n",
    "# Load the prediction dataset\n",
    "prediction_file=f\"data/dataset_nomogram.csv\"\n",
    "prediction_data=pd.read_csv(prediction_file)\n",
    "\n",
    "# Extract features from the prediction data\n",
    "X_pred=prediction_data.drop(columns=['outcome','outcome_onset','outcome_weight'])\n",
    "\n",
    "# Make predictions on the prediction data\n",
    "y_pred_proba=best_model.predict_proba(X_pred)[:,1]\n",
    "\n",
    "# Output model evaluation\n",
    "print(f\"Algorithm: {algorithm_name}, Prediction file: {prediction_file}\")\n",
    "\n",
    "# Write the predicted probabilities to CSV\n",
    "output_filename=os.path.join(save_dir,f\"prob_{os.path.basename(prediction_file)}\")\n",
    "pd.DataFrame({'prob':y_pred_proba}).to_csv(output_filename,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
